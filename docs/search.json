[
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "Note: This page is a hub for academic papers that utilize NLP or related techniques with a strong social science focus. Please cite them in Chicago format."
  },
  {
    "objectID": "literature.html#word-embedding",
    "href": "literature.html#word-embedding",
    "title": "Literature",
    "section": "Word embedding",
    "text": "Word embedding\nMethodology\n\nRODRIGUEZ, PEDRO L., ARTHUR SPIRLING, and BRANDON M. STEWART. “Embedding Regression: Models for Context-Specific Description and Inference.” American Political Science Review 117, no. 4 (2023): 1255–74. Link\n\nApplication\n\n…"
  },
  {
    "objectID": "literature.html#text-classification",
    "href": "literature.html#text-classification",
    "title": "Literature",
    "section": "Text classification",
    "text": "Text classification\nMethodology\n\nLaurer, Moritz, Wouter van Atteveldt, Andreu Casas, and Kasper Welbers. “Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI.” Political Analysis 32, no. 1 (2024): 84–100. Link\n\nApplication\n\n…"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "“Hands-on Transformers: Fine-Tune your own BERT and GPT.” Data Science Summer School 2023 by Moritz Laurer, with Code and Video."
  },
  {
    "objectID": "resources.html#online-course",
    "href": "resources.html#online-course",
    "title": "Resources",
    "section": "",
    "text": "“Hands-on Transformers: Fine-Tune your own BERT and GPT.” Data Science Summer School 2023 by Moritz Laurer, with Code and Video."
  },
  {
    "objectID": "resources.html#cheat-sheet",
    "href": "resources.html#cheat-sheet",
    "title": "Resources",
    "section": "Cheat sheet",
    "text": "Cheat sheet\n\nString manipulation with stringr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The Power of Singular Vector Decomposition: A Beginner’s Guide\n\n\n\n\n\n\n\ncode\n\n\nR\n\n\nMachine Learning\n\n\nPrincipal Component Analysis\n\n\nTopic Modeling\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nZhaowen Guo\n\n\n\n\n\n\n  \n\n\n\n\nZero-shot classification with Hugging Face\n\n\n\n\n\n\n\ncode\n\n\nR\n\n\nPython\n\n\nHugging Face\n\n\nTransformers\n\n\nText Classification\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nBrian Leung\n\n\n\n\n\n\n  \n\n\n\n\nHow to write a blog post with R or Python code?\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nBrian Leung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html",
    "href": "posts/022224BL/GettingStarted.html",
    "title": "How to write a blog post with R or Python code?",
    "section": "",
    "text": "Welcome to P-DAWG! This post introduces us to using Quarto (.qmd) to write a blog-style document that integrates R or Python code with text!"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#quarto-.qmd-is-just-like-rmarkdown-.rmd",
    "href": "posts/022224BL/GettingStarted.html#quarto-.qmd-is-just-like-rmarkdown-.rmd",
    "title": "How to write a blog post with R or Python code?",
    "section": "Quarto (.qmd) is just like RMarkdown (.rmd)!",
    "text": "Quarto (.qmd) is just like RMarkdown (.rmd)!\nYou can open a new Quarto Document (.qmd) in Rstudio and choose HTML as the format. This works just like a traditional RMarkdown file (.rmd).\nYou can copy and paste the following code to the preamble (i.e., top of the document) and fill out the details:\n---\ntitle: \"Give it a nice title\"\nauthor: \"Your good name\"\ndate: \"today is?\"\ncategories: [tag1, tag2, tag3...] \nmessage: false \nwarning: false\n---"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#working-with-r-code",
    "href": "posts/022224BL/GettingStarted.html#working-with-r-code",
    "title": "How to write a blog post with R or Python code?",
    "section": "Working with R code",
    "text": "Working with R code\nInside the .qmd document, you can type text anywhere you like. To insert a code chunk, type “/” to select options from a drop-down menu (if you are using Visual editor mode). For example, I can insert the following R code chunk:\n\nlibrary(tidyverse)\n\nThen you can perform data analysis as usual:\n\ndata(iris)\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\n\nVisualization is also easy:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#working-with-python-code",
    "href": "posts/022224BL/GettingStarted.html#working-with-python-code",
    "title": "How to write a blog post with R or Python code?",
    "section": "Working with Python code",
    "text": "Working with Python code\nQuarto document has the advantage of being able to run Python code at the same time. But this does requiries some knowledge about the language, and especially how virtual environment works for managing packages.\nTo do that from a RStudio environment, you have to first install reticulate package in R by running install.packages(\"reticulate\").\nAfter loading reticulate, you can create a virtual environment and install the required Python packages:\n\nlibrary(reticulate)\n\n# create a new environment \nvirtualenv_create(\"r-reticulate\")\n\nvirtualenv: r-reticulate\n\n# packages to install \npackages &lt;- c(\"numpy\", \"pandas\")\n\n# install packages to the environment\nvirtualenv_install(\"r-reticulate\", packages)\n\nUsing virtual environment 'r-reticulate' ...\n\n\nYou might have to go to Options &gt; Python &gt; Virtual Environment, and then select the correct Python interpreter.\nNow, you can run Python code! One advantage of doing it in the Quarto document (with reticulate package) is that you can reference and transport R object directly into the Python environment by calling r.object_name:\n\nimport numpy as np \nimport pandas as pd\n\niris = r.iris # transport R object into Python\niris_groupMean = iris.groupby(['Species']).mean()\niris_groupMean\n\n            Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\nSpecies                                                         \nsetosa             5.006        3.428         1.462        0.246\nversicolor         5.936        2.770         4.260        1.326\nvirginica          6.588        2.974         5.552        2.026\n\n\nYou can do the reverse by accessing Python object in the R environment by calling py$object_name:\n\nglimpse(py$iris_groupMean) # back to R code\n\nRows: 3\nColumns: 4\n$ Sepal.Length &lt;dbl&gt; 5.006, 5.936, 6.588\n$ Sepal.Width  &lt;dbl&gt; 3.428, 2.770, 2.974\n$ Petal.Length &lt;dbl&gt; 1.462, 4.260, 5.552\n$ Petal.Width  &lt;dbl&gt; 0.246, 1.326, 2.026"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#conclusion",
    "href": "posts/022224BL/GettingStarted.html#conclusion",
    "title": "How to write a blog post with R or Python code?",
    "section": "Conclusion",
    "text": "Conclusion\nQuarto document .qmd gives us an easy way to write a blog post that seamlessly integrates narrative text with code. As long as you submit a minimally reproducible Quarto document (that you can Render your document successfully), your post can be published in no time.\nFurthermore, the reticulate package enables interoperability between R and Python code in the same document. Many state-of-the-art models and packages in NLP are written in Python. Hopefully, this will encourage R users to take advantage of Python packages in a familiar environment."
  },
  {
    "objectID": "posts/051624ZG/svd.html",
    "href": "posts/051624ZG/svd.html",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "",
    "text": "SVD is not nearly as famous as it should be. — Gilbert Strang"
  },
  {
    "objectID": "posts/051624ZG/svd.html#introduction",
    "href": "posts/051624ZG/svd.html#introduction",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Introduction",
    "text": "Introduction\nSingular Vector Decomposition (SVD) is a matrix factorization technique that has become a cornerstone in the field of machine learning (ML). It not only allows for efficiently calculating the inverse of a matrix (if it exists) by multiplying the inverse of each decomposed simpler matrices, but also opens the door to a wide array of applications in ML and beyond.\nIn what follows, I will start by the definition and properties of SVD, and establish its connection with Principal Component Analysis (PCA). Then I will demonstrate different applications of SVD in ML, including but not limited to missing value imputation and latent feature extraction.\n\nDefinition and properties of SVD\nSVD decomposes a data matrix \\(X_{m \\times n}\\) into three matrices \\(U_{m\\times r}\\), \\(D_{r\\times r}\\), and \\(V_{n\\times r}\\), regardless of the characteristics of the original matrix.\n\\[\nX = UDV^T\n\\] where\n\nU and V are orthogonal matrices (\\(U^T U = I\\) and \\(V^T V = I\\)), which are called left singular vector, and right singular vector, respectively\nD is a diagonal matrix with non-negative and decreasing elements, which are called singular values\n\n\n\n\nImage credit: Tai-Danae Bradley at Math3ma\n\n\nLet’s first check dimensions of the resulting matrices after applying SVD to a toy matrix X.\n\n# Define a matrix\nX &lt;- matrix(c(1:12),\n            nrow = 4,\n            ncol = 3,\n            byrow = T)\n\n# Apply SVD\nsvd_result &lt;- svd(X)\n\n# Extract U, D, and V matrices\nU &lt;- svd_result$u\nD &lt;- diag(svd_result$d)\nV &lt;- svd_result$v\nprint(paste0(\"The dimension for U matrix: \", dim(U)[1], \" X \", dim(U)[2]))\n\n[1] \"The dimension for U matrix: 4 X 3\"\n\nprint(paste0(\"The dimension for D matrix: \", dim(D)[1], \" X \", dim(D)[2]))\n\n[1] \"The dimension for D matrix: 3 X 3\"\n\nprint(paste0(\"The dimension for V matrix: \", dim(V)[1], \" X \", dim(V)[2]))\n\n[1] \"The dimension for V matrix: 3 X 3\"\n\n\nWe can then check matrix properties of SVD. As we can observe, matrices U and V are orthogonal, and matrix D is diagonal.\n\n# Check properties of U and V (orthogonal matrix)\nis_orthogonal &lt;- function(A){\n  A_T &lt;- t(A)\n  dot_product_1 &lt;- A %*% A_T\n  dot_product_2 &lt;- A_T %*% A\n  identity_matrix_1 &lt;- diag(nrow(A))\n  identity_matrix_2 &lt;- diag(ncol(A))\n  \n  result &lt;- isTRUE(all.equal(dot_product_1, identity_matrix_1)) +\n            isTRUE(all.equal(dot_product_2, identity_matrix_2)) # all.equal checks \"nearly equal\"\n  \n  return(result&gt;=1)\n}\n\nis_orthogonal(U) # TRUE\n\n[1] TRUE\n\nis_orthogonal(V) # TRUE\n\n[1] TRUE\n\n# Check properties of D\ndiag(D) # diagonal values (or singular values in this case)\n\n[1] 2.546241e+01 1.290662e+00 1.809728e-15\n\nD[!row(D) == col(D)] # off-diagonal values are 0\n\n[1] 0 0 0 0 0 0\n\n\n\n\nConnection between SVD and PCA\nNow, knowing that SVD can be used to approximate any matrix, it’s an opportune moment to revisit Principal Component Analysis (PCA), an unsupervised ML method that we might be more familiar with. As we will see, SVD on a de-meaned (centered) data matrix is the same as PCA.\nRecall that PCA seeks to find principal components, or the direction in the feature space with maximum variance in the data.\n\n# Center the data matrix (column means are 0)\nX_centered &lt;- scale(X, center = T, scale = T)\ncolMeans(X_centered) # check if centered\n\n[1] 0 0 0\n\n# Apply SVD to the centered data matrix\nsvd_result &lt;- svd(X_centered)\n\n# Apply PCA to the data\npca_result &lt;- prcomp(X, scale. = T)\n\nAs we can see, columns of the right singular vector V correspond to principal components extracted from PCA, and SVD also yields less elapsed time than PCA. A key advantage of SVD is that it does not require a preliminary step of constructing a covariance as PCA does, providing greater computational efficiency in extracting principal components.\nThis efficiency becomes particularly prominent when handling\n\nHigh-dimensional datasets: when a data matrix possess too many features, the computational cost for constructing its covariance matrix can be huge\nFull-rank data matrix: when the data matrix is full-rank, it often implies that many singular values will be non-negligible, and many principal components will be needed to reconstruct the original matrix\n\n\nprint(svd_result$v) # right singular vectors\n\n          [,1]       [,2]       [,3]\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\nprint(pca_result$rotation) # principal components\n\n           PC1        PC2        PC3\n[1,] 0.5773503 -0.8164966  0.0000000\n[2,] 0.5773503  0.4082483 -0.7071068\n[3,] 0.5773503  0.4082483  0.7071068\n\n\n\n# Construct a high-dimensional and sparse data matrix\nn_rows &lt;- 1000\nn_cols &lt;- 500\n\nsparse_matrix &lt;- matrix(0, nrow = n_rows, ncol = n_cols)\n\n# Manually add some non-zero elements to mimic sparsity\nset.seed(123)\nnon_zero_elements &lt;- 200\nfor (i in 1:non_zero_elements) {\n  row_index &lt;- sample(n_rows, 1)\n  col_index &lt;- sample(n_cols, 1)\n  sparse_matrix[row_index, col_index] &lt;- runif(1)\n}\n\n\n# Compute every possible rank approximations\nsystem.time({\n  svd_result &lt;- svd(sparse_matrix)\n})\n\n   user  system elapsed \n  0.618   0.002   0.629 \n\nsystem.time({\n  pca_res &lt;- prcomp(sparse_matrix)\n})\n\n   user  system elapsed \n  0.691   0.002   0.696 \n\n\n\n# Compute top 10 rank approximations\nsvd_time &lt;- proc.time()\nsvd_result &lt;- irlba(sparse_matrix, nv = 10)\nproc.time() - svd_time\n\n   user  system elapsed \n  0.039   0.000   0.039 \n\npca_time &lt;- proc.time()\npca_res &lt;- prcomp(sparse_matrix, rank. = 10)\nproc.time() - pca_time\n\n   user  system elapsed \n  0.611   0.002   0.614"
  },
  {
    "objectID": "posts/051624ZG/svd.html#application-impute-missing-values",
    "href": "posts/051624ZG/svd.html#application-impute-missing-values",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Impute Missing Values",
    "text": "Application: Impute Missing Values\nOne popular application of SVD is to impute missing values. Without keeping all singular values and vectors, we can just retain the first d largest singular values to approximate the matrix A. The intuition is that the approximated matrix \\(A_d\\) being a dense matrix that captures the primary structure and patterns in the original data.\nThis procedure is also called lower-rank approximation, which can be implemented in the following steps:\n\nMatrix approximation: fill in NAs with an initial guess (e.g. column means, zeros) and apply SVD with rank d, meaning that we only keep top d singular values and vectors\nMissingness imputation: use the approximated matrix \\(A_d\\) to fill in NAs in the original matrix\n\nLet’s use the following example for illustration:\nWe start by creating a toy data matrix A and call it our ground truth matrix. Then we manually add sparsity by replacing certain elements with NAs.\n\n# Create a toy dataset (sparse matrix)\nset.seed(123)\nA &lt;- matrix(sample(c(NA, 1:5), 25, replace = T), 5, 5)\nground_truth_matrix &lt;- A\nA[c(2, 8, 10, 14, 20)] &lt;- NA\n\n\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]   NA    2   NA    2    4\n[3,]    2   NA    1   NA    2\n[4,]    1    3   NA    3    1\n[5,]    1   NA    4   NA    1\n\nground_truth_matrix\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    5    5    2   NA\n[2,]    5    2   NA    2    4\n[3,]    2    4    1   NA    2\n[4,]    1    3    2    3    1\n[5,]    1    5    4   NA    1\n\n\nNext, we apply SVD with varying d, which indicates the number of singular values/vectors.\n\n# Define svd\nimpute_svd &lt;- function(matrix, d){\n  \n  # fill in missingness with column means\n  column_means &lt;- colMeans(matrix, na.rm = T)\n  matrix_filled &lt;- matrix\n  na_indices &lt;- is.na(matrix)\n  matrix_filled[na_indices] &lt;- column_means[col(matrix)[na_indices]]\n  \n  # perform svd\n  svd_res &lt;- svd(matrix_filled)\n  svd_res$d &lt;- c(svd_res$d[1:d], rep(0, length(svd_res$d) - d))\n  \n  # reconstruct the matrix\n  approx_matrix &lt;- svd_res$u %*% diag(svd_res$d) %*% t(svd_res$v)\n  imputed_vals &lt;- approx_matrix\n  imputed_vals[!is.na(matrix)] &lt;- NA\n  return(imputed_vals)\n}\n\nWe can use the approximated matrix \\(A_d\\) to reconstruct the original matrix and impute missing values. We can evaluate the performance of missingness imputation by mean squared error (MSE).\n\n# Construct the metric MSE\nmse &lt;- function(predicted, truth) {\n  mean((predicted - truth)^2, na.rm = TRUE)\n}\n\n# Display MSE for different d values (rank, or number of dimensions to define the reduced matrix)\nsvd_errors &lt;- numeric(5)\nfor (d in 1:5) {\n  imputed_values &lt;- impute_svd(A, d)\n  svd_errors[d] &lt;- mse(imputed_values, ground_truth_matrix)\n}\n\nHow does SVD perform? As a baseline, consider a simple approach by replacing missing values with column means. It seems that rank-2 approximation is an optimal choice, which yields the lowest MSE. However, it’s important to note that it is not always the case that SVD approximation would outperform simple column mean imputation. We might need to consider other matrix decomposition techniques for missingness imputation, such as Non-negative Matrix Factorization (NMF), Alternating Least Squares (ALS), etc..\n\n# Create baseline imputation from column means\nna_indices &lt;- is.na(A)\ncolmean_matrix &lt;- A\ncolmean_matrix[na_indices] &lt;- colMeans(A, na.rm = T)[col(A)[na_indices]]\ncolmean_errors &lt;- mse(colmean_matrix[na_indices], ground_truth_matrix[na_indices])\n\n# Report comparison of performance\ncomparison &lt;- tibble(\"Method\" = c(\"Column Means\", \n                                  \"Rank-1 Approximation\",\n                                  \"Rank-2 Approximation\",\n                                  \"Rank-3 Approximation\",\n                                  \"Rank-4 Approximation\",\n                                  \"Rank-5 Approximation\"),\n                     \"MSE\" = c(colmean_errors, svd_errors))\n\ncomparison %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nMethod\nMSE\n\n\n\n\nColumn Means\n4.312500\n\n\nRank-1 Approximation\n4.754839\n\n\nRank-2 Approximation\n4.094591\n\n\nRank-3 Approximation\n4.146353\n\n\nRank-4 Approximation\n4.319335\n\n\nRank-5 Approximation\n4.312500\n\n\n\n\n\n\n\nIn line with the idea of missingness imputation, SVD can also be leveraged to enhance recommendation systems! The goal is to predict unknown preferences or ratings of users for items (e.g. movies, products, or services) based on existing ratings. A notable example is Netflix Prize competition, where Netflix offered $1 million award to anyone who could improve the accuracy of its movie recommendation system by 10%. The winning team just used SVD, along with techniques that incorporate other metadata, achieving a 10.06% improvement!"
  },
  {
    "objectID": "posts/051624ZG/svd.html#application-topic-modeling",
    "href": "posts/051624ZG/svd.html#application-topic-modeling",
    "title": "The Power of Singular Vector Decomposition: A Beginner’s Guide",
    "section": "Application: Topic Modeling",
    "text": "Application: Topic Modeling\nSVD is a powerful and generalizable technique that provides us another perspective on topic modeling. We begin by first transforming documents into a document-term matrix, where each row represents a document, each column reflects a term, and each cell denotes frequency. To refine this step further, we can also apply Term Frequency-Inverse Document Frequency (TF-IDF) to reweigh the cell values, adjusting for the uniqueness of each term for a given document.\nSVD can then be perceived as decomposing a document-term matrix \\(X_{m \\times n}\\) into\n\n\\(U_{m \\times r}\\): document-topic matrix\n\\(D_{r \\times r}\\): diagonal elements represent topic importance\n\\(V_{n \\times r}\\): term-topic matrix\n\nFor topic modeling, a crucial hyperparameter that requires tuning is the number of topics (often denoted by k). In the context of SVD, the idea is equivalent to selecting the top k singular values and their corresponding singular vectors in order to approximate the original data matrix.\n\n# Construct a document-term matrix\nlibrary(tidytext)\nlibrary(tm)\ndocuments &lt;- tibble(\n  doc_id = 1:8,\n  text = c(\"The sky is blue and beautiful.\",\n           \"Love this blue and beautiful sky!\",\n           \"The quick brown fox jumps over the lazy dog.\",\n           \"A king's breakfast has sausages, ham, bacon, eggs, toast, and beans\",\n           \"I love green eggs, ham, sausages, and bacon!\",\n           \"The brown fox is quick and the blue dog is lazy!\",\n           \"The sky is very blue and the sky is very beautiful today\",\n           \"The dog is lazy but the brown fox is quick!\")\n)\n\ntidy_documents &lt;- documents %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  anti_join(stop_words)\n\ndtm &lt;- tidy_documents %&gt;%\n  count(doc_id, word) %&gt;%\n  cast_dtm(doc_id, word, n)\n\n\n# Apply SVD and examine each decomposed matrix\nsvd_result &lt;- svd(as.matrix(dtm))\n\nk &lt;- 2 # choose k=2 for simplicity\nUk &lt;- svd_result$u[, 1:k]\nDk &lt;- svd_result$d[1:k]\nVk &lt;- svd_result$v[, 1:k]\n\nAs we can see, the decomposed \\(U_k\\) matrix captures documents by topics.\n\nUs &lt;- tibble(`Document ID` = 1:8,\n             `Topic 1` = Uk[,1],\n             `Topic 2` = Uk[,2])\nUs %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nDocument ID\nTopic 1\nTopic 2\n\n\n\n\n1\n-0.1294362\n0.4303175\n\n\n2\n-0.1392703\n0.4926330\n\n\n3\n-0.5597761\n-0.1933906\n\n\n4\n-0.0088357\n0.2551944\n\n\n5\n-0.0175544\n0.2534139\n\n\n6\n-0.5889438\n-0.0537521\n\n\n7\n-0.1672636\n0.6091753\n\n\n8\n-0.5246738\n-0.1772371\n\n\n\n\n\n\n\nThe singular values are stored in the following matrix \\(D_k\\), which correspond to how important each topic is.\n\nD_matrix &lt;- diag(Dk)\nrownames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\ncolnames(D_matrix) &lt;- c(\"Topic 1\", \"Topic 2\")\n\nD_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\n\nTopic 1\nTopic 2\n\n\n\n\nTopic 1\n3.993368\n0.000000\n\n\nTopic 2\n0.000000\n3.460071\n\n\n\n\n\n\n\nThe \\(V_k\\) matrix represents terms by topics.\n\nterms &lt;- colnames(dtm)\nV_matrix &lt;- tibble(`Term` = terms,\n                   `Topic 1` = Vk[,1],\n                   `Topic 2` = Vk[,2])\n\nV_matrix %&gt;%\n  kbl() %&gt;%\n  kable_styling()\n\n\n\n\nTerm\nTopic 1\nTopic 2\n\n\n\n\nbeautiful\n-0.1091735\n0.4428019\n\n\nblue\n-0.2566540\n0.4272669\n\n\nsky\n-0.1510589\n0.6188605\n\n\nlove\n-0.0392713\n0.2156161\n\n\nbrown\n-0.4190432\n-0.1226506\n\n\ndog\n-0.4190432\n-0.1226506\n\n\nfox\n-0.4190432\n-0.1226506\n\n\njumps\n-0.1401764\n-0.0558921\n\n\nlazy\n-0.4190432\n-0.1226506\n\n\nquick\n-0.4190432\n-0.1226506\n\n\nbacon\n-0.0066085\n0.1469936\n\n\nbeans\n-0.0022126\n0.0737541\n\n\nbreakfast\n-0.0022126\n0.0737541\n\n\neggs\n-0.0066085\n0.1469936\n\n\nham\n-0.0066085\n0.1469936\n\n\nking's\n-0.0022126\n0.0737541\n\n\nsausages\n-0.0066085\n0.1469936\n\n\ntoast\n-0.0022126\n0.0737541\n\n\ngreen\n-0.0043959\n0.0732395\n\n\n\n\n\n\n\nNow, we can examine top 5 terms associated with each topic.\n\ntop_terms &lt;- apply(Vk, 2, function(x) terms[order(abs(x), decreasing = TRUE)[1:5]])\nprint(top_terms)\n\n     [,1]    [,2]       \n[1,] \"fox\"   \"sky\"      \n[2,] \"brown\" \"beautiful\"\n[3,] \"dog\"   \"blue\"     \n[4,] \"lazy\"  \"love\"     \n[5,] \"quick\" \"bacon\"    \n\n\nBeyond what has been discussed, some other cool applications of SVD in NLP include: information retrieval via Latent Semantic Analysis and word co-occurrence detection in word embeddings and other downstream tasks (e.g. text classification). Feel free to explore!\nReferences and additional resources:\n\nA wonderful twitter thread on SVD by Daniela Witten (a nice summary can be found here)\nA cool geometric interpretation of SVD\nA nice tutorial illustrating the connection between SVD and topic modeling using Python"
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#why-hugging-face",
    "href": "posts/040424BL/huggingFace.html#why-hugging-face",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Why Hugging Face?",
    "text": "Why Hugging Face?\nAdvances in natural language processing (NLP), particularly with the advent of of large language models (LLMs), have created exciting opportunities for social science researchers to deal with a large amount of text as data. But numerous barriers to entry existed: the knowledge, data, and computational resources required to train and fine-tune the models to specific tasks can be very daunting for us.\nSo, there is a gap between what NLP models or resources are available out there and what we as social scientists can reasonably digest and incorporate into our workflow. Researchers with a technical comparative advantage in training and fine-tuning models have already produced resources that have immense potentials for social science applications.\nFor example, PoliBERTweet is a pre-trained BERT model – a transformer-based model, much like its cousin GPT (“Generative Pre-trained Transformer”). It is pre-trained in the sense that it was trained on 83 million politics-related Tweets, making it suitable for a wide range of downstream, domain-specific tasks related to politics. But the problem is, how we as social scientists can take advantage of such readily available resources?\nThere is where Hugging Face comes into play. Much like Github, it is a community platform that allows practitioners and researchers to host and collaborate on AI models. Many state-of-the-art NLP models are available for specific downstream tasks, like text classification (e.g., for sentiment analysis or topic classification) or embedding documents to compare their similarity.\nMost importantly, it comes with a Python package – transformers – that makes downloading and implementing those pre-trained models super easy and dramatically lowers the entry cost. But it does require some knowledge in Python."
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#how-to-get-started-as-a-r-user",
    "href": "posts/040424BL/huggingFace.html#how-to-get-started-as-a-r-user",
    "title": "Zero-shot classification with Hugging Face",
    "section": "How to get started as a R user?",
    "text": "How to get started as a R user?\nIn this post, I want to develop a workflow that centers on a R environment (e.g., writing a .rmd/.qmd, or wrangling data with tidyverse) that feels familiar to us, but one that incorporates the power of Python packages like transformers only when we need to.\nI can’t tell you how much the fear and discomfort from an interrupted workflow – switching from one language to a less-familiar one, and transporting objects between different interfaces – have discouraged people (myself included) from taking advantage of Python.\nHopefully, an integrated workflow that makes R and Python interoperable will remove the last barrier to entry to unleash the power of NLP in our research."
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#setting-up-python-in-r-with-reticulate",
    "href": "posts/040424BL/huggingFace.html#setting-up-python-in-r-with-reticulate",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Setting up Python in R with reticulate",
    "text": "Setting up Python in R with reticulate\nFirst, let’s set up a virtual environment to install the required Python packages – particularly transformers via the reticulate package in R:\n\nlibrary(reticulate)\n\nvirtualenv_create(\"r-reticulate\")\n\nvirtualenv: r-reticulate\n\npackages &lt;- c(\"transformers==4.37.2\", \"tensorflow\", \"torch\", \"torchvision\", \"torchaudio\")\n\nvirtualenv_install(\"r-reticulate\", packages)\n\nUsing virtual environment 'r-reticulate' ...\n\n\nIf it is the first time for you to install the packages, it might take some time as they are quite large in size."
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#basic-text-classification-with-transformers",
    "href": "posts/040424BL/huggingFace.html#basic-text-classification-with-transformers",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Basic text classification with transformers",
    "text": "Basic text classification with transformers\nTo see if you have installed the packages and selected the correct Python interpreter, run the following code to import pipeline, the key function from transformers:\n\nfrom transformers import pipeline\n\nNow, we can take advantage of pre-trained models on Hugging Face and perform text analyses. It can be done in a few lines of code. But you must first define the language task you want to perform and select the corresponding model. For example, I can perform sentiment analysis on a text by running:\n\nclassifier = pipeline(task = \"sentiment-analysis\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\ntext = \"This blog post is not unhelpful\"\noutput = classifier(text)\nprint(output)\n\n[{'label': 'POSITIVE', 'score': 0.7062975168228149}]\n\n\nThe sentiment classifier assigns a positive label to my double-negative sentence, which is reasonable. More generically, in pipeline(...), you have to declare the task (e.g., “sentiment-analysis”) and the model. The default model “distilbert/distilbert-base-uncased-finetuned-sst-2-english” is chosen because the user doesn’t specify one, which is not a recommended practice. You can go to Hugging Face to look for specific models for your particular NLP tasks. Be aware that NLP models tend to be quite large in size (some gigabytes), so it can take a while for your first time installation."
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#classifying-political-stances-with-transformers",
    "href": "posts/040424BL/huggingFace.html#classifying-political-stances-with-transformers",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Classifying political stances with transformers",
    "text": "Classifying political stances with transformers\nThe following section showcases a DeBERTa-based model trained for stance detection, first by Laurer et al and further improved on by Michael Burnham. Behind the model, there is an interesting literature called natural language inference (NLI) or textual entailment. This is suitable for detecting political or issue stances behind some text in a zero-shot setting (i.e., the model can make prediction on arbitrary labels it wasn’t trained on but we care about).\nTo perform political stance detection:\n\nzeroshot_classifier = pipeline(\"zero-shot-classification\", model = \"mlburnham/deberta-v3-large-polistance-affect-v1.0\")\ntext = \"Many American jobs are shipped to Chinese factories.\"\nhypothesis_template = \"This text supports trading {} with China\"\nclasses_verbalized = [\"more\", \"less\"]\noutput = zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)\nprint(output)\n\n{'sequence': 'Many American jobs are shipped to Chinese factories.', 'labels': ['less', 'more'], 'scores': [0.9999904632568359, 9.514573321212083e-06]}\n\n\nThe classifier looks at the text and perform hypothesis testings: does the text (based on “common” understanding of the language) entail one hypothesis (e.g., it supports trading more with China) or the other (e.g., trading less with China)? It assigns probabilities to each hypothesis and the label with the highest probability is chosen (multiple labels are allowed as an option though). For example, the classifier correctly identify the text (“Many American jobs are shipped to Chinese factories.”) as a statement that supports trading less with China."
  },
  {
    "objectID": "posts/040424BL/huggingFace.html#bonus",
    "href": "posts/040424BL/huggingFace.html#bonus",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Bonus",
    "text": "Bonus\nTo transport the result back to R for wrangling:\n\nlibrary(tidyverse)\noutput &lt;- py$output\noutput %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(id_cols = sequence, names_from = labels, values_from = scores)\n\n# A tibble: 1 × 3\n  sequence                                              less       more\n  &lt;chr&gt;                                                &lt;dbl&gt;      &lt;dbl&gt;\n1 Many American jobs are shipped to Chinese factories.  1.00 0.00000951\n\n\nTo suppress with a warning in Python:\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nTo enable GPU:\n\n# import torch\n# if torch.backends.mps.is_available():\n#     mps_device = torch.device(\"mps\")\n#     x = torch.ones(1, device=mps_device)\n#     print (x)\n# else:\n#     print (\"MPS device not found.\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "We meet every two weeks in Smith 40A from 3:00 to 4:00 pm Pacific Time. The format is hybrid (Zoom link here; only invited users are allowed). Please consult the schedule below for the details of presentations:\n\n\n\n\n\n\n\n\nDate\nPresenter\nProposed topic\n\n\n\n\nApril 4 / Week 2\nBrian Leung\nWelcoming / Zero-shot classification with Hugging Face & Transformers\n\n\nApril 18 / Week 4\nJihyeon Bae\nLimitations of BERT and Promises of Topic Modeling with UN General Debate Data\n\n\nMay 2 / Week 6\nTao Lin\n“Relatio: Text Semantics Capture Political and Economic Narratives” by Ash et al. (2024)\n\n\nMay 16 / Week 8\nZhaowen Guo\nFine-tuning Pre-trained Language Models Using PyTorch\n\n\nMay 30 / Week 10\nNela Mrchkovska\nFine-tuning Topic Classification Models with Sermons Data"
  }
]