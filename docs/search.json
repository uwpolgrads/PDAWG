[
  {
    "objectID": "literature.html",
    "href": "literature.html",
    "title": "Literature",
    "section": "",
    "text": "Note: This page is a hub for academic papers that utilize NLP or related techniques with a strong social science focus. Please cite them in Chicago format."
  },
  {
    "objectID": "literature.html#word-embedding",
    "href": "literature.html#word-embedding",
    "title": "Literature",
    "section": "Word embedding",
    "text": "Word embedding\nMethodology\n\nRODRIGUEZ, PEDRO L., ARTHUR SPIRLING, and BRANDON M. STEWART. “Embedding Regression: Models for Context-Specific Description and Inference.” American Political Science Review 117, no. 4 (2023): 1255–74. Link\n\nApplication\n\n…"
  },
  {
    "objectID": "literature.html#text-classification",
    "href": "literature.html#text-classification",
    "title": "Literature",
    "section": "Text classification",
    "text": "Text classification\nMethodology\n\nLaurer, Moritz, Wouter van Atteveldt, Andreu Casas, and Kasper Welbers. “Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI.” Political Analysis 32, no. 1 (2024): 84–100. Link\n\nApplication\n\n…"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "“Hands-on Transformers: Fine-Tune your own BERT and GPT.” Data Science Summer School 2023 by Moritz Laurer, with Code and Video."
  },
  {
    "objectID": "resources.html#online-course",
    "href": "resources.html#online-course",
    "title": "Resources",
    "section": "",
    "text": "“Hands-on Transformers: Fine-Tune your own BERT and GPT.” Data Science Summer School 2023 by Moritz Laurer, with Code and Video."
  },
  {
    "objectID": "resources.html#cheat-sheet",
    "href": "resources.html#cheat-sheet",
    "title": "Resources",
    "section": "Cheat sheet",
    "text": "Cheat sheet\n\nString manipulation with stringr"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Zero-shot classification with Hugging Face\n\n\n\n\n\n\n\ncode\n\n\nR\n\n\nPython\n\n\nhugging face\n\n\ntext classification\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nBrian Leung\n\n\n\n\n\n\n  \n\n\n\n\nHow to write a blog post with R or Python code?\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nBrian Leung\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html",
    "href": "posts/022224BL/GettingStarted.html",
    "title": "How to write a blog post with R or Python code?",
    "section": "",
    "text": "Welcome to P-DAWG! This post introduces us to using Quarto (.qmd) to write a blog-style document that integrates R or Python code with text!"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#quarto-.qmd-is-just-like-rmarkdown-.rmd",
    "href": "posts/022224BL/GettingStarted.html#quarto-.qmd-is-just-like-rmarkdown-.rmd",
    "title": "How to write a blog post with R or Python code?",
    "section": "Quarto (.qmd) is just like RMarkdown (.rmd)!",
    "text": "Quarto (.qmd) is just like RMarkdown (.rmd)!\nYou can open a new Quarto Document (.qmd) in Rstudio and choose HTML as the format. This works just like a traditional RMarkdown file (.rmd).\nYou can copy and paste the following code to the preamble (i.e., top of the document) and fill out the details:\n---\ntitle: \"Give it a nice title\"\nauthor: \"Your good name\"\ndate: \"today is?\"\ncategories: [tag1, tag2, tag3...] \nmessage: false \nwarning: false\n---"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#working-with-r-code",
    "href": "posts/022224BL/GettingStarted.html#working-with-r-code",
    "title": "How to write a blog post with R or Python code?",
    "section": "Working with R code",
    "text": "Working with R code\nInside the .qmd document, you can type text anywhere you like. To insert a code chunk, type “/” to select options from a drop-down menu (if you are using Visual editor mode). For example, I can insert the following R code chunk:\n\nlibrary(tidyverse)\n\nThen you can perform data analysis as usual:\n\ndata(iris)\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(across(where(is.numeric), mean))\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\n\nVisualization is also easy:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#working-with-python-code",
    "href": "posts/022224BL/GettingStarted.html#working-with-python-code",
    "title": "How to write a blog post with R or Python code?",
    "section": "Working with Python code",
    "text": "Working with Python code\nQuarto document has the advantage of being able to run Python code at the same time. But this does requiries some knowledge about the language, and especially how virtual environment works for managing packages.\nTo do that from a RStudio environment, you have to first install reticulate package in R by running install.packages(\"reticulate\").\nAfter loading reticulate, you can create a virtual environment and install the required Python packages:\n\nlibrary(reticulate)\n\n# create a new environment \nvirtualenv_create(\"r-reticulate\")\n\nvirtualenv: r-reticulate\n\n# packages to install \npackages &lt;- c(\"numpy\", \"pandas\")\n\n# install packages to the environment\nvirtualenv_install(\"r-reticulate\", packages)\n\nUsing virtual environment 'r-reticulate' ...\n\n\nYou might have to go to Options &gt; Python &gt; Virtual Environment, and then select the correct Python interpreter.\nNow, you can run Python code! One advantage of doing it in the Quarto document (with reticulate package) is that you can reference and transport R object directly into the Python environment by calling r.object_name:\n\nimport numpy as np \nimport pandas as pd\n\niris = r.iris # transport R object into Python\niris_groupMean = iris.groupby(['Species']).mean()\niris_groupMean\n\n            Sepal.Length  Sepal.Width  Petal.Length  Petal.Width\nSpecies                                                         \nsetosa             5.006        3.428         1.462        0.246\nversicolor         5.936        2.770         4.260        1.326\nvirginica          6.588        2.974         5.552        2.026\n\n\nYou can do the reverse by accessing Python object in the R environment by calling py$object_name:\n\nglimpse(py$iris_groupMean) # back to R code\n\nRows: 3\nColumns: 4\n$ Sepal.Length &lt;dbl&gt; 5.006, 5.936, 6.588\n$ Sepal.Width  &lt;dbl&gt; 3.428, 2.770, 2.974\n$ Petal.Length &lt;dbl&gt; 1.462, 4.260, 5.552\n$ Petal.Width  &lt;dbl&gt; 0.246, 1.326, 2.026"
  },
  {
    "objectID": "posts/022224BL/GettingStarted.html#conclusion",
    "href": "posts/022224BL/GettingStarted.html#conclusion",
    "title": "How to write a blog post with R or Python code?",
    "section": "Conclusion",
    "text": "Conclusion\nQuarto document .qmd gives us an easy way to write a blog post that seamlessly integrates narrative text with code. As long as you submit a minimally reproducible Quarto document (that you can Render your document successfully), your post can be published in no time.\nFurthermore, the reticulate package enables interoperability between R and Python code in the same document. Many state-of-the-art models and packages in NLP are written in Python. Hopefully, this will encourage R users to take advantage of Python packages in a familiar environment."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#why-hugging-face",
    "href": "posts/022324BL/huggingFace.html#why-hugging-face",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Why Hugging Face?",
    "text": "Why Hugging Face?\nAdvances in natural language processing (NLP), particularly with the advent of of large language models (LLMs), have created exciting opportunities for social science researchers to deal with a large amount of text as data. But numerous barriers to entry existed: the knowledge, data, and computational resources required to train and fine-tune the models to specific tasks can be very daunting for us.\nSo, there is a gap between what NLP models or resources are available out there and what we as social scientists can reasonably digest and incorporate into our workflow. Researchers with a technical comparative advantage in training and fine-tuning models have already produced resources that have immense potentials for social science applications.\nFor example, PoliBERTweet is a pre-trained BERT model – a transformer-based model, much like its cousin GPT (“Generative Pre-trained Transformer”). It is pre-trained in the sense that it was trained on 83 million politics-related Tweets, making it suitable for a wide range of downstream, domain-specific tasks related to politics. But the problem is, how we as social scientists can take advantage of such readily available resources?\nThere is where Hugging Face comes into play. Much like Github, it is a community platform that allows practitioners and researchers to host and collaborate on AI models. Many state-of-the-art NLP models are available for specific downstream tasks, like text classification (e.g., for sentiment analysis or topic classification) or embedding documents to compare their similarity.\nMost importantly, it comes with a Python package – transformers – that makes downloading and implementing those pre-trained models super easy and dramatically lowers the entry cost. But it does require some knowledge in Python."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#how-to-get-started-as-a-r-user",
    "href": "posts/022324BL/huggingFace.html#how-to-get-started-as-a-r-user",
    "title": "Zero-shot classification with Hugging Face",
    "section": "How to get started as a R user?",
    "text": "How to get started as a R user?\nIn this post, I want to develop a workflow that centers on a R environment (e.g., writing a .rmd/.qmd, or wrangling data with tidyverse) that feels familiar to us, but one that incorporates the power of Python packages like transformers only when we need to.\nI can’t tell you how much the fear and discomfort from an interrupted workflow – switching from one language to a less-familiar one, and transporting objects between different interfaces – have discouraged people (myself included) from taking advantage of Python.\nHopefully, an integrated workflow that makes R and Python interoperable will remove the last barrier to entry to unleash the power of NLP in our research."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#setting-up-python-in-r-with-reticulate",
    "href": "posts/022324BL/huggingFace.html#setting-up-python-in-r-with-reticulate",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Setting up Python in R with reticulate",
    "text": "Setting up Python in R with reticulate\nFirst, let’s set up a virtual environment to install the required Python packages – particularly transformers via the reticulate package in R:\n\nlibrary(reticulate)\n\nvirtualenv_create(\"r-reticulate\")\n\nvirtualenv: r-reticulate\n\npackages &lt;- c(\"transformers\", \"tensorflow\", \"torch\", \"torchvision\", \"torchaudio\")\n\nvirtualenv_install(\"r-reticulate\", packages)\n\nUsing virtual environment 'r-reticulate' ...\n\n\nIf it is the first time for you to install the packages, it might take some time as they are quite large in size."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#basic-text-classification-with-transformers",
    "href": "posts/022324BL/huggingFace.html#basic-text-classification-with-transformers",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Basic text classification with transformers",
    "text": "Basic text classification with transformers\nTo see if you have installed the packages and selected the correct Python interpreter, run the following code to import pipeline, the key function from transformers:\n\nfrom transformers import pipeline\n\nNow, we can take advantage of pre-trained models on Hugging Face and perform text analyses. It can be done in a few lines of code. But you must first define the language task you want to perform and select the corresponding model. For example, I can perform sentiment analysis on a text by running:\n\nclassifier = pipeline(task = \"sentiment-analysis\")\n\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\ntext = \"This blog post is not unhelpful\"\noutput = classifier(text)\nprint(output)\n\n[{'label': 'POSITIVE', 'score': 0.7062975168228149}]\n\n\nThe sentiment classifier assigns a positive label to my double-negative sentence, which is reasonable. More generically, in pipeline(...), you have to declare the task (e.g., “sentiment-analysis”) and the model. The default model “distilbert/distilbert-base-uncased-finetuned-sst-2-english” is chosen because the user doesn’t specify one, which is not a recommended practice. You can go to Hugging Face to look for specific models for your particular NLP tasks. Be aware that NLP models tend to be quite large in size (some gigabytes), so it can take a while for your first time installation."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#classifying-political-stances-with-transformers",
    "href": "posts/022324BL/huggingFace.html#classifying-political-stances-with-transformers",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Classifying political stances with transformers",
    "text": "Classifying political stances with transformers\nThe following section showcases a DeBERTa-based model trained for stance detection, first by Laurer et al and further improved on by Michael Burnham. Behind the model, there is an interesting literature called natural language inference (NLI) or textual entailment. This is suitable for detecting political or issue stances behind some text in a zero-shot setting (i.e., the model can make prediction on arbitrary labels it wasn’t trained on but we care about).\nTo perform political stance detection:\n\nzeroshot_classifier = pipeline(\"zero-shot-classification\", model = \"mlburnham/deberta-v3-large-polistance-affect-v1.0\")\ntext = \"Many American jobs are shipped to Chinese factories.\"\nhypothesis_template = \"This text supports trading {} with China\"\nclasses_verbalized = [\"more\", \"less\"]\noutput = zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)\nprint(output)\n\n{'sequence': 'Many American jobs are shipped to Chinese factories.', 'labels': ['less', 'more'], 'scores': [0.9999904632568359, 9.514573321212083e-06]}\n\n\nThe classifier looks at the text and perform hypothesis testings: does the text (based on “common” understanding of the language) entail one hypothesis (e.g., it supports trading more with China) or the other (e.g., trading less with China)? It assigns probabilities to each hypothesis and the label with the highest probability is chosen (multiple labels are allowed as an option though). For example, the classifier correctly identify the text (“Many American jobs are shipped to Chinese factories.”) as a statement that supports trading less with China."
  },
  {
    "objectID": "posts/022324BL/huggingFace.html#bonus",
    "href": "posts/022324BL/huggingFace.html#bonus",
    "title": "Zero-shot classification with Hugging Face",
    "section": "Bonus",
    "text": "Bonus\nTo transport the result back to R for wrangling:\n\nlibrary(tidyverse)\noutput &lt;- py$output\noutput %&gt;%\n  bind_rows() %&gt;%\n  pivot_wider(id_cols = sequence, names_from = labels, values_from = scores)\n\n# A tibble: 1 × 3\n  sequence                                              less       more\n  &lt;chr&gt;                                                &lt;dbl&gt;      &lt;dbl&gt;\n1 Many American jobs are shipped to Chinese factories.  1.00 0.00000951\n\n\nTo deal with a warning:\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nTo enable GPU:\n\n# import torch\n# if torch.backends.mps.is_available():\n#     mps_device = torch.device(\"mps\")\n#     x = torch.ones(1, device=mps_device)\n#     print (x)\n# else:\n#     print (\"MPS device not found.\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Date\nPresenter\nProposed topic\n\n\n\n\nApril 4 / Week 2\nBrian Leung\nWelcoming / Code showcase: Zero-shot classification with Hugging Face & Transformers\n\n\nApril 18 / Week 4\nJihyeon Bae\nWork in progress / Code showcase: Limitations of BERT and Promises of Topic Modeling with UN General Debate dataset\n\n\nMay 2 / Week 6\nZhaowen Guo\nCode showcase: Fine-tuning pre-trained language models using PyTorch\n\n\nMay 16 / Week 8\nTao Lin\nCode showcase: “Relatio: Text Semantics Capture Political and Economic Narratives” by Ash et al. (2024)\n\n\nMay 30 / Week 10\nTBD\nTBD"
  }
]