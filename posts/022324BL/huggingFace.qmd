---
title: "Zero-shot classification with Hugging Face"
author: "Brian Leung"
date: "2024-02-23"
categories: [code, R, Python, hugging face, text classification] 
message: false 
warning: false
---

![](hf-logo-with-title.png){fig-align="center"}

## Why Hugging Face?

Advances in natural language processing (NLP), particularly with the advent of of large language models (LLMs), have created exciting opportunities for social science researchers to deal with a large amount of text as data. But numerous barriers to entry existed: the knowledge, data, and computational resources required to train and fine-tune the models to specific tasks can be very daunting for us.

So, there is a *gap* between what NLP models or resources are available out there and what we as social scientists can reasonably digest and incorporate into our workflow. Researchers with a technical comparative advantage in training and fine-tuning models have already produced resources that have immense potentials for social science applications.

For example, [PoliBERTweet](https://aclanthology.org/2022.lrec-1.801/) is a pre-trained BERT model – a transformer-based model, much like its cousin GPT ("Generative Pre-trained *Transformer*"). It is pre-trained in the sense that it was trained on 83 million politics-related Tweets, making it suitable for a wide range of downstream, domain-specific tasks related to politics. But the problem is, how we as social scientists can take advantage of such readily available resources?

There is where [Hugging Face](https://huggingface.co/) comes into play. Much like Github, it is a community platform that allows practitioners and researchers to host and collaborate on AI models. Many state-of-the-art NLP models are available for specific downstream tasks, like text classification (e.g., for sentiment analysis or topic classification) or embedding documents to compare their similarity.

Most importantly, it comes with a `Python` package – `transformers` – that makes downloading and implementing those pre-trained models super easy and dramatically lowers the entry cost. But it does require some knowledge in `Python`.

## How to get started as a R user?

In this post, I want to develop a workflow that centers on a `R` environment (e.g., writing a `.rmd`/`.qmd`, or wrangling data with `tidyverse`) that feels familiar to us, but one that incorporates the power of `Python` packages like `transformers` *only when we need to*.

I can't tell you how much the fear and discomfort from an interrupted workflow – switching from one language to a less-familiar one, and transporting objects between different interfaces – have discouraged people (myself included) from taking advantage of `Python`.

Hopefully, an integrated workflow that makes `R` and `Python` interoperable will remove the last barrier to entry to unleash the power of NLP in our research.

## Setting up Python in R with `reticulate`

First, let's set up a virtual environment to install the required `Python` packages – particularly `transformers` via the `reticulate` package in `R`:

```{r}
library(reticulate)

virtualenv_create("r-reticulate")

packages <- c("transformers", "tensorflow", "torch", "torchvision", "torchaudio")

virtualenv_install("r-reticulate", packages)
```

If it is the first time for you to install the packages, it might take some time as they are quite large in size.

## Basic text classification with `transformers`

To see if you have installed the packages and selected the correct `Python` interpreter, run the following code to import `pipeline`, the key function from `transformers`:

```{python}
from transformers import pipeline
```

Now, we can take advantage of pre-trained models on Hugging Face and perform text analyses. It can be done in *a few lines of code*. But you must first define the *language task* you want to perform and select the corresponding *model.* For example, I can perform sentiment analysis on a text by running:

```{python}
classifier = pipeline(task = "sentiment-analysis")
text = "This blog post is not unhelpful"
output = classifier(text)
print(output)

```

The sentiment classifier assigns a positive label to my double-negative sentence, which is reasonable. More generically, in `pipeline(...)`, you have to declare the task (e.g., "sentiment-analysis") and the model. The default model "distilbert/distilbert-base-uncased-finetuned-sst-2-english" is chosen because the user doesn't specify one, which is not a recommended practice. You can go to [Hugging Face](https://huggingface.co/models) to look for specific models for your particular NLP tasks. Be aware that NLP models tend to be quite large in size (some gigabytes), so it can take a while for your first time installation.

## Classifying political stances with `transformers`

The following section showcases a DeBERTa-based model trained for stance detection, first by [Laurer et al](https://huggingface.co/MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33) and further improved on by [Michael Burnham](https://huggingface.co/mlburnham/deberta-v3-large-polistance-affect-v1.0). Behind the model, there is an interesting literature called natural language inference (NLI) or textual entailment. This is suitable for detecting political or issue stances behind some text in a zero-shot setting (i.e., the model can make prediction on arbitrary labels it wasn't trained on but we care about).

To perform political stance detection:

```{python}
zeroshot_classifier = pipeline("zero-shot-classification", model = "mlburnham/deberta-v3-large-polistance-affect-v1.0")
text = "Many American jobs are shipped to Chinese factories."
hypothesis_template = "This text supports trading {} with China"
classes_verbalized = ["more", "less"]
output = zeroshot_classifier(text, classes_verbalized, hypothesis_template=hypothesis_template, multi_label=False)
print(output)
```

The classifier looks at the text and perform hypothesis testings: does the text (based on "common" understanding of the language) *entail* one hypothesis (e.g., it supports trading more with China) or the other (e.g., trading less with China)? It assigns probabilities to each hypothesis and the label with the highest probability is chosen (multiple labels are allowed as an option though). For example, the classifier correctly identify the text ("Many American jobs are shipped to Chinese factories.") as a statement that supports trading less with China.

## Bonus 

To transport the result back to `R` for wrangling:

```{r}
library(tidyverse)
output <- py$output
output %>%
  bind_rows() %>%
  pivot_wider(id_cols = sequence, names_from = labels, values_from = scores)
```

To suppress with a warning in `Python`:

```{python}
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```

To enable GPU:

```{python}
# import torch
# if torch.backends.mps.is_available():
#     mps_device = torch.device("mps")
#     x = torch.ones(1, device=mps_device)
#     print (x)
# else:
#     print ("MPS device not found.")

```
